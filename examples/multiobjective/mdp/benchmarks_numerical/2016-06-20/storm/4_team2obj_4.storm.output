SToRM
---------------

Version: 0.9.1 (+1781 commits) build from revision g5578a73 (DIRTY).

Linked with GNU Linear Programming Kit v4.57.
Linked with SMT-RAT 2.1.0.
Linked with CARL.
Command line arguments: -s team/team2obj_4.nm -prop team/team2obj_4_numerical.pctl --precision 0.000001 --multiobjective:precision 0.0001 
Current working directory: /Users/tim/storm/examples/multi-objective/mdp

-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	96665
Transitions: 	116464
Choices: 	115289
Reward Models:  w_1_total
Labels: 	2
   * init -> 1 state(s)
   * ((((status = 6) & ((t1_r1 = 1) => (((((m1_t1 = 1) & (1 = 1)) | ((m2_t1 = 1) & (2 = 1))) | ((m3_t1 = 1) & (3 = 1))) | ((m4_t1 = 1) & (1 = 1))))) & ((t1_r2 = 1) => (((((m1_t1 = 1) & (1 = 2)) | ((m2_t1 = 1) & (2 = 2))) | ((m3_t1 = 1) & (3 = 2))) | ((m4_t1 = 1) & (1 = 2))))) & ((t1_r3 = 1) => (((((m1_t1 = 1) & (1 = 3)) | ((m2_t1 = 1) & (2 = 3))) | ((m3_t1 = 1) & (3 = 3))) | ((m4_t1 = 1) & (1 = 3))))) -> 4872 state(s)
choice labels: 	no
Size in memory: 3996 kbytes
-------------------------------------------------------------- 

Model checking property: multi(Pmax=? [F ((((status = 6) & ((t1_r1 = 1) => (((((m1_t1 = 1) & (1 = 1)) | ((m2_t1 = 1) & (2 = 1))) | ((m3_t1 = 1) & (3 = 1))) | ((m4_t1 = 1) & (1 = 1))))) & ((t1_r2 = 1) => (((((m1_t1 = 1) & (1 = 2)) | ((m2_t1 = 1) & (2 = 2))) | ((m3_t1 = 1) & (3 = 2))) | ((m4_t1 = 1) & (1 = 2))))) & ((t1_r3 = 1) => (((((m1_t1 = 1) & (1 = 3)) | ((m2_t1 = 1) & (2 = 3))) | ((m3_t1 = 1) & (3 = 3))) | ((m4_t1 = 1) & (1 = 3)))))], R[exp]{"w_1_total"}>=2.423469388 [C]) ...
---------------------------------------------------------------------------------------------------------------------------------------
                                                       Multi-objective Preprocessor Data                                               
---------------------------------------------------------------------------------------------------------------------------------------

Original Formula: 
--------------------------------------------------------------
	multi(Pmax=? [F ((((status = 6) & ((t1_r1 = 1) => (((((m1_t1 = 1) & (1 = 1)) | ((m2_t1 = 1) & (2 = 1))) | ((m3_t1 = 1) & (3 = 1))) | ((m4_t1 = 1) & (1 = 1))))) & ((t1_r2 = 1) => (((((m1_t1 = 1) & (1 = 2)) | ((m2_t1 = 1) & (2 = 2))) | ((m3_t1 = 1) & (3 = 2))) | ((m4_t1 = 1) & (1 = 2))))) & ((t1_r3 = 1) => (((((m1_t1 = 1) & (1 = 3)) | ((m2_t1 = 1) & (2 = 3))) | ((m3_t1 = 1) & (3 = 3))) | ((m4_t1 = 1) & (1 = 3)))))], R[exp]{"w_1_total"}>=2.423469388 [C])

Objectives:
--------------------------------------------------------------
Pmax=? [F ((((status = 6) & ((t1_r1 = 1) => (((((m1_t1 = 1) & (1 = 1)) | ((m2_t1 = 1) & (2 = 1))) | ((m3_t1 = 1) & (3 = 1))) | ((m4_t1 = 1) & (1 = 1))))) & ((t1_r2 = 1) => (((((m1_t1 = 1) & (1 = 2)) | ((m2_t1 = 1) & (2 = 2))) | ((m3_t1 = 1) & (3 = 2))) | ((m4_t1 = 1) & (1 = 2))))) & ((t1_r3 = 1) => (((((m1_t1 = 1) & (1 = 3)) | ((m2_t1 = 1) & (2 = 3))) | ((m3_t1 = 1) & (3 = 3))) | ((m4_t1 = 1) & (1 = 3)))))] 	(toOrigVal:  1*x +  0, 	intern threshold:   none, 	intern reward model: objective1 (positive), 	step bound: none)
R[exp]{"w_1_total"}>=2.42347 [C] 	(toOrigVal:  1*x +  0, 	intern threshold:>=2.423469388, 	intern reward model: objective2 (positive), 	step bound: none)
--------------------------------------------------------------

Original Model Information:
-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	96665
Transitions: 	116464
Choices: 	115289
Reward Models:  w_1_total
Labels: 	2
   * init -> 1 state(s)
   * ((((status = 6) & ((t1_r1 = 1) => (((((m1_t1 = 1) & (1 = 1)) | ((m2_t1 = 1) & (2 = 1))) | ((m3_t1 = 1) & (3 = 1))) | ((m4_t1 = 1) & (1 = 1))))) & ((t1_r2 = 1) => (((((m1_t1 = 1) & (1 = 2)) | ((m2_t1 = 1) & (2 = 2))) | ((m3_t1 = 1) & (3 = 2))) | ((m4_t1 = 1) & (1 = 2))))) & ((t1_r3 = 1) => (((((m1_t1 = 1) & (1 = 3)) | ((m2_t1 = 1) & (2 = 3))) | ((m3_t1 = 1) & (3 = 3))) | ((m4_t1 = 1) & (1 = 3))))) -> 4872 state(s)
choice labels: 	no
Size in memory: 3996 kbytes
-------------------------------------------------------------- 

Preprocessed Model Information:
-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	96665
Transitions: 	116464
Choices: 	115289
Reward Models:  objective1, objective2
Labels: 	3
   * prob1 -> 0 state(s)
   * init -> 1 state(s)
   * ((((status = 6) & ((t1_r1 = 1) => (((((m1_t1 = 1) & (1 = 1)) | ((m2_t1 = 1) & (2 = 1))) | ((m3_t1 = 1) & (3 = 1))) | ((m4_t1 = 1) & (1 = 1))))) & ((t1_r2 = 1) => (((((m1_t1 = 1) & (1 = 2)) | ((m2_t1 = 1) & (2 = 2))) | ((m3_t1 = 1) & (3 = 2))) | ((m4_t1 = 1) & (1 = 2))))) & ((t1_r3 = 1) => (((((m1_t1 = 1) & (1 = 3)) | ((m2_t1 = 1) & (2 = 3))) | ((m3_t1 = 1) & (3 = 3))) | ((m4_t1 = 1) & (1 = 3))))) -> 4872 state(s)
choice labels: 	no
Size in memory: 4557 kbytes
-------------------------------------------------------------- 

---------------------------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------------------------
                                                 Multi-objective Model Checking Statistics:                                            
---------------------------------------------------------------------------------------------------------------------------------------

Runtimes (in seconds):
	 Preprocessing:       9.398
	 Value Iterations:    0.094
	 Postprocessing:          0
	 Combined:            9.493

Performed Refinement Steps: 1

---------------------------------------------------------------------------------------------------------------------------------------
 done.
Result (initial states): [false]
===== Statistics ==============================
peak memory usage: 228MB
CPU time: 5.227 seconds
===============================================
OVERALL_TIME; 10.955
